{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-Current-GPT2-generate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNeXNKF6Zv59"
      },
      "source": [
        "# GPT-2 Text Generation \n",
        "\n",
        "Made by [Artem Konevskikh](https://aiculedssul.net/)\n",
        "\n",
        "Based on notebook by [Max Woolf](http://minimaxir.com). For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM69Hoc5j27B"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-du42C9dZsEy",
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "#@markdown By running this cell we are loading libraries needed to work with GPT2\n",
        "!pip install git+https://github.com/minimaxir/gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWDuRBNnMyu"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "There are two ways of generating text with GPT-2 by using either standard pretrained models or those you finetuned on your texts.\n",
        "\n",
        "In this notebook we will use standard models, but we will play with finetuning later (in another workshop)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxA3S-zhofDl",
        "cellView": "form"
      },
      "source": [
        "#@title Load Pretrained Model\n",
        "\n",
        "#@markdown There are four released sizes of GPT-2 that we can use to generate text in Colab:\n",
        "\n",
        "#@markdown * `124M` (default): the \"small\" model, 500MB on disk.\n",
        "#@markdown * `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "#@markdown * `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model\n",
        "#@markdown * `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. Also, like `774M`, it cannot be finetuned in Colab.\n",
        "\n",
        "#@markdown Larger model has more knowledge, but takes longer to generate text. You can specify which base model to use by changing `model_name`.\n",
        "\n",
        "model_name = \"124M\" #@param [\"124M\", \"355M\", \"774M\", \"1558M\"] {allow-input: true}\n",
        "\n",
        "#@markdown This cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "#@markdown This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time.\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "\n",
        "#@markdown *__Select the model and run the cell to load it__*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIjEWLn2qKbG",
        "cellView": "form"
      },
      "source": [
        "#@title Generation with the pretrained models\n",
        "#@markdown **Generation parameters**\n",
        "\n",
        "#@markdown You can pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there\n",
        "prefix = 'You might ask what digital persona is. There is ' #@param {type: \"string\"}\n",
        "#@markdown Number of tokens to generate (default 1023, the maximum)\n",
        "length = 200  #@param {type:\"slider\", min:1, max:1023, step:1}\n",
        "#@markdown The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "temperature=0.8  #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "#@markdown Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "top_k=0  #@param {type: \"number\"}\n",
        "#@markdown Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "top_p=0.9  #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown Number of samples to generate\n",
        "nsamples=10  #@param {type: \"number\"}\n",
        "#@markdown Number of samples to generate in pararallel to speed up the process\n",
        "batch_size=5  #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "#@markdown Save samples to text file\n",
        "save_to_file = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown *__Set parameters and  and run the cell to generate samples__*\n",
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow()) if save_to_file else None\n",
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              destination_path=gen_file,\n",
        "              prefix=None if prefix=='' else prefix,\n",
        "              length=length,\n",
        "              temperature=temperature,\n",
        "              top_k=int(top_k),\n",
        "              top_p=top_p,\n",
        "              nsamples=int(nsamples),\n",
        "              batch_size=batch_size\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "r2bnL3olUJWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86af8fde-cc01-4fbe-dac6-a9727261537a"
      },
      "source": [
        "#@markdown **Download newest generated file**\n",
        "if gen_file:\n",
        "  files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4f7616eb-5fff-41a6-bf71-0c4045d3d2e6\", \"gpt2_gentext_20210413_090528.txt\", 48846)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}