{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One Click Jukebox with Autosave v2.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAE4EmJ0d9yZ"
      },
      "source": [
        "#(Almost) One-click Jukebox notebook with autosaving.\n",
        "\n",
        "Speed upsampling supported. Switch to upsample mode will happen automatically if data file is detected within the folder provided.\n",
        "\n",
        "Colab Pro users can use the 5b_lyrics (or 5b) model.\n",
        "Free users can also use the 5b_lyrics (or 5b) model, if they are assigned a Tesla T4 GPU. If assigned a Tesla K80, the weaker 1b_lyrics model is recommended.\n",
        "\n",
        "Join the Jukebox community at https://discord.gg/aEqXFN9amV\n",
        "\n",
        "Big thank you to Michaels Lab for all the cool new stuff: https://www.youtube.com/user/CraftMine1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qEqdj8u0gdN",
        "cellView": "form"
      },
      "source": [
        "#@title Check which GPU you were assigned by running this cell.\n",
        "!nvidia-smi -L\n",
        "your_lyrics = \"\"\"\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_lyrics = \"\"\"\n",
        "We're no strangers to love\n",
        "You know the rules and so do I\n",
        "A full commitment's what I'm thinking of\n",
        "You wouldn't get this from any other guy\n",
        "\n",
        "I just wanna tell you how I'm feeling\n",
        "Gotta make you understand\n",
        "\n",
        "Never gonna give you up\n",
        "Never gonna let you down\n",
        "Never gonna run around and desert you\n",
        "Never gonna make you cry\n",
        "Never gonna say goodbye\n",
        "Never gonna tell a lie and hurt you\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AoXdBHkX4d1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAdFGF-bqVMY",
        "cellView": "form"
      },
      "source": [
        "#@title Select your settings and run this cell to start generating\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install --upgrade git+https://github.com/craftmine1000/jukebox-saveopt.git\n",
        "\n",
        "import jukebox\n",
        "import torch as t\n",
        "import librosa\n",
        "import os\n",
        "from IPython.display import Audio\n",
        "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
        "from jukebox.hparams import Hyperparams, setup_hparams\n",
        "from jukebox.sample import sample_single_window, _sample, \\\n",
        "                           sample_partial_window, upsample, \\\n",
        "                           load_prompts\n",
        "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
        "from jukebox.utils.torch_utils import empty_cache\n",
        "# MPI Connect. MPI doesn't like being initialized twice, hence the following\n",
        "try:\n",
        "    if device is not None:\n",
        "        pass\n",
        "except NameError:\n",
        "    rank, local_rank, device = setup_dist_from_mpi()\n",
        "\n",
        "model = \"5b_lyrics\" #@param [\"5b_lyrics\", \"5b\", \"1b_lyrics\"]\n",
        "if model == '5b':\n",
        "  your_lyrics = \"\"\"\n",
        "  \"\"\"\n",
        "hps = Hyperparams()\n",
        "hps.sr = 44100\n",
        "hps.n_samples =  2#@param {type:\"integer\"}\n",
        "# Specifies the directory to save the sample in.\n",
        "# We set this to the Google Drive mount point.\n",
        "hps.name = '/content/gdrive/MyDrive/Your_folder' #@param {type:\"string\"}\n",
        "chunk_size = 16 if model in ('5b', '5b_lyrics') else 32\n",
        "gpu_info = !nvidia-smi -L\n",
        "if gpu_info[0].find('Tesla T4') >= 0:\n",
        "  max_batch_size = 2\n",
        "  print('Tesla T4 detected, max_batch_size set to 2')\n",
        "elif gpu_info[0].find('Tesla K80') >= 0:\n",
        "  max_batch_size = 8\n",
        "  print('Tesla K80 detected, max_batch_size set to 8')\n",
        "elif gpu_info[0].find('Tesla P100') >= 0:\n",
        "  max_batch_size = 3\n",
        "  print('Tesla P100 detected, max_batch_size set to 3')\n",
        "elif gpu_info[0].find('Tesla V100') >= 0:\n",
        "  max_batch_size = 3\n",
        "  print('Tesla V100 detected, max_batch_size set to 3')\n",
        "else:\n",
        "  max_batch_size = 3\n",
        "  print('Different GPU detected, max_batch_size set to 3.')\n",
        "hps.levels = 3\n",
        "speed_upsampling = True #@param {type: \"boolean\"}\n",
        "if speed_upsampling == True:\n",
        "  hps.hop_fraction = [1,1,.125]\n",
        "else:\n",
        "  hps.hop_fraction = [.5,.5,.125]\n",
        "\n",
        "vqvae, *priors = MODELS[model]\n",
        "vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), device)\n",
        "top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)\n",
        "\n",
        "# The default mode of operation.\n",
        "# Creates songs based on artist and genre conditioning.\n",
        "mode = 'primed' #@param [\"ancestral\", \"primed\"]\n",
        "if mode == 'ancestral':\n",
        "  codes_file=None\n",
        "  audio_file=None\n",
        "  prompt_length_in_seconds=None\n",
        "if mode == 'primed':\n",
        "  codes_file=None\n",
        "  # Specify an audio file here.\n",
        "  audio_file = '/content/gdrive/MyDrive/your_file.wav' #@param {type:\"string\"}\n",
        "  # Specify how many seconds of audio to prime on.\n",
        "  prompt_length_in_seconds=10 #@param {type:\"integer\"}\n",
        "\n",
        "sample_length_in_seconds = 70 #@param {type:\"integer\"}\n",
        "\n",
        "if os.path.exists(hps.name):\n",
        "  # Identify the lowest level generated and continue from there.\n",
        "  for level in [0, 1, 2]:\n",
        "    data = f\"{hps.name}/level_{level}/data.pth.tar\"\n",
        "    if os.path.isfile(data):\n",
        "      codes_file = data\n",
        "      if int(sample_length_in_seconds) > int(librosa.get_duration(filename=f'{hps.name}/level_2/item_0.wav')):\n",
        "        mode = 'continue'\n",
        "      else:\n",
        "        mode = 'upsample'\n",
        "      break\n",
        "\n",
        "print('mode is now '+mode)\n",
        "if mode == 'continue':\n",
        "  print('Continuing from level 2')\n",
        "if mode == 'upsample':\n",
        "  print('Upsampling from level '+str(level))\n",
        "\n",
        "sample_hps = Hyperparams(dict(mode=mode, codes_file=codes_file, audio_file=audio_file, prompt_length_in_seconds=prompt_length_in_seconds))\n",
        "\n",
        "if mode == 'upsample':\n",
        "  sample_length_in_seconds=int(librosa.get_duration(filename=f'{hps.name}/level_{level}/item_0.wav'))\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zs = [z.cpu() for z in data['zs']]\n",
        "  hps.n_samples = zs[-1].shape[0]\n",
        "\n",
        "if mode == 'continue':\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zs = [z.cpu() for z in data['zs']]\n",
        "  hps.n_samples = zs[-1].shape[0]\n",
        "\n",
        "hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "assert hps.sample_length >= top_prior.n_ctx*top_prior.raw_to_tokens, f'Please choose a larger sampling rate'\n",
        "\n",
        "# Note: Metas can contain different prompts per sample.\n",
        "# By default, all samples use the same prompt.\n",
        "\n",
        "select_artist = \"the beatles\" #@param {type:\"string\"}\n",
        "select_genre = \"rock\" #@param {type:\"string\"}\n",
        "metas = [dict(artist = select_artist,\n",
        "            genre = select_genre,\n",
        "            total_length = hps.sample_length,\n",
        "            offset = 0,\n",
        "            lyrics = your_lyrics, \n",
        "            ),\n",
        "          ] * hps.n_samples\n",
        "labels = [None, None, top_prior.labeller.get_batch_labels(metas, 'cuda')]\n",
        "\n",
        "sampling_temperature = .98 #@param {type:\"number\"}\n",
        "\n",
        "if gpu_info[0].find('Tesla T4') >= 0:\n",
        "  lower_batch_size = 12\n",
        "  print('Tesla T4 detected, lower_batch_size set to 12')\n",
        "elif gpu_info[0].find('Tesla K80') >= 0:\n",
        "  lower_batch_size = 8\n",
        "  print('Tesla K80 detected, lower_batch_size set to 8')\n",
        "elif gpu_info[0].find('Tesla P100') >= 0:\n",
        "  lower_batch_size = 16\n",
        "  print('Tesla P100 detected, lower_batch_size set to 16')\n",
        "elif gpu_info[0].find('Tesla V100') >= 0:\n",
        "  lower_batch_size = 16\n",
        "  print('Tesla V100 detected, lower_batch_size set to 16')\n",
        "else:\n",
        "  lower_batch_size = 8\n",
        "  print('Different GPU detected, lower_batch_size set to 8.')\n",
        "lower_level_chunk_size = 32\n",
        "chunk_size = 16 if model in ('5b', '5b_lyrics') else 32\n",
        "sampling_kwargs = [dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n",
        "                        chunk_size=lower_level_chunk_size),\n",
        "                    dict(temp=0.99, fp16=True, max_batch_size=lower_batch_size,\n",
        "                         chunk_size=lower_level_chunk_size),\n",
        "                    dict(temp=sampling_temperature, fp16=True, \n",
        "                         max_batch_size=max_batch_size, chunk_size=chunk_size)]\n",
        "\n",
        "if sample_hps.mode == 'ancestral':\n",
        "  zs = [t.zeros(hps.n_samples,0,dtype=t.long, device='cpu') for _ in range(len(priors))]\n",
        "  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "elif sample_hps.mode == 'upsample':\n",
        "  assert sample_hps.codes_file is not None\n",
        "  # Load codes.\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zs = [z.cpu() for z in data['zs']]\n",
        "  assert zs[-1].shape[0] == hps.n_samples, f\"Expected bs = {hps.n_samples}, got {zs[-1].shape[0]}\"\n",
        "  del data\n",
        "  print('One click upsampling!')\n",
        "elif sample_hps.mode == 'primed':\n",
        "  assert sample_hps.audio_file is not None\n",
        "  audio_files = sample_hps.audio_file.split(',')\n",
        "  duration = (int(sample_hps.prompt_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "  x = load_prompts(audio_files, duration, hps)\n",
        "  zs = top_prior.encode(x, start_level=0, end_level=len(priors), bs_chunks=x.shape[0])\n",
        "  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "elif sample_hps.mode == 'continue':\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zs = [z.cuda() for z in data['zs']]\n",
        "  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "else:\n",
        "  raise ValueError(f'Unknown sample mode {sample_hps.mode}.')\n",
        "\n",
        "# Set this False if you are on a local machine that has enough memory (this allows you to do the\n",
        "# lyrics alignment visualization during the upsampling stage). For a hosted runtime, \n",
        "# we'll need to go ahead and delete the top_prior if you are using the 5b_lyrics model.\n",
        "if True:\n",
        "  del top_prior\n",
        "  empty_cache()\n",
        "  top_prior=None\n",
        "upsamplers = [make_prior(setup_hparams(prior, dict()), vqvae, 'cpu') for prior in priors[:-1]]\n",
        "labels[:2] = [prior.labeller.get_batch_labels(metas, 'cuda') for prior in upsamplers]\n",
        "\n",
        "zs = upsample(zs, labels, sampling_kwargs, [*upsamplers, top_prior], hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide to the above settings:\n",
        "\n",
        "**your_lyrics:** Specify the lyrics Jukebox should attempt to follow. You can paste any lyrics you want in here or leave it blank, which will result in gibberish.\n",
        "\n",
        "**model:**\n",
        "OpenAI has trained a few different models for Jukebox. In this notebook, you can access the 5b_lyrics, 5b and 1b_lyrics models. As you can imagine, the 5b_lyrics model is the superior one, but also requires a stronger GPU to run properly. Which model you should choose depends on the GPU you were assigned, which you can check in the first cell of the notebook. Recommended settings: 5b_lyrics on P100 or T4 GPU, 1b_lyrics on K80 GPU.\n",
        "(5b_lyrics theoretically works on a K80 now, but sampling is going to be super slow.)\n",
        "(5b is like 5b_lyrics, without supporting custom lyrics, so it will generate gibberish lyrics)\n",
        "\n",
        "**hps.n_samples:**\n",
        "Here you can choose how many samples you want to generate. Different GPUs can handle a different amount of samples. Recommended settings:\n",
        "P100 GPU: 3 samples,\n",
        "T4 GPU: 2 samples;\n",
        "K80 GPU: up to 8 samples, but 1b_lyrics only.\n",
        "\n",
        "**hps.name:** Specifies the name of the folder in Google Drive, where you will find your results in. Make sure to choose a different name for each of your runs, or else the notebook will get confused.\n",
        "\n",
        "**speed_upsampling:** If selected, will upsample much faster, at the cost of the samples sounding slightly \"choppy\". \n",
        "\n",
        "**mode:** Available modes are primed and ancestral. Primed will continue an already existing song, ancestral generates a song from scratch. (Upsample mode will be selected automatically if a data file is detected within the folder provided)\n",
        "\n",
        "**audio_file:** Only needed for primed mode. Specifies which song Jukebox will continue. Upload the file you want (needs to be .wav format!) to the root directory of your Google Drive and fill in its name above.\n",
        "\n",
        "**prompt_length_in_seconds:** Only needed for primed mode. Specifies how many seconds of your file Jukebox will be primed on (so, at which point Jukebox  will \"kick in\"). Recommended to keep below 24 seconds for memory reasons.\n",
        "\n",
        "**sample_length_in_seconds:** Specifies how long your fully generated samples are going to be.\n",
        "\n",
        "**select_artist and select_genre:** List of available artists and genres can be found here: https://github.com/openai/jukebox/tree/master/jukebox/data/ids\n",
        "The 5b_lyrics (and 5b) model utilizes the v2 lists, the 1b_lyrics model the v3 lists. It is possible to combine up to five v2 genres, for example \"Hip Hop Pop Punk Disco\". Combining v3 genres is not possible.\n",
        "\n",
        "**sampling_temperature:** Determines the creativity and energy of Jukebox. The higher the temperature, the more chaotic and intense the result will be. You can experiment with this. Recommended to keep between .96 and .999"
      ],
      "metadata": {
        "id": "ZCSxNemzcMxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important links:\n",
        "\n",
        "Official blog: https://openai.com/blog/jukebox/\n",
        "Original repo: https://github.com/openai/jukebox/\n",
        "\n",
        "License: Non-commercial, for details see: https://github.com/openai/jukebox/blob/master/LICENSE"
      ],
      "metadata": {
        "id": "Lu12YwYVDbJ3"
      }
    }
  ]
}