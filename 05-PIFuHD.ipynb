{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05-Current-PIFuHD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eclLG4xlJRIE"
      },
      "source": [
        "# PIFuHD \n",
        "\n",
        "![](https://shunsukesaito.github.io/PIFuHD/resources/images/pifuhd.gif)\n",
        "\n",
        "Made by [Artem Konevskikh](https://aiculedssul.net/)\n",
        "\n",
        "Based on notebook by [![Follow](https://img.shields.io/twitter/follow/psyth91?style=social)](https://twitter.com/psyth91)\n",
        "\n",
        "To see how the model works, visit project [official page](https://shunsukesaito.github.io/PIFuHD/) or repository [![GitHub stars](https://img.shields.io/github/stars/facebookresearch/pifuhd?style=social)](https://github.com/facebookresearch/pifuhd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUEXAvcvkVYV"
      },
      "source": [
        "## Tips for Inputs: My results are broken!\n",
        "\n",
        "(Kudos to those who share results on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag!!!!)\n",
        "\n",
        "Due to the limited variation in the training data, your results might be broken sometimes. Here I share some useful tips to get resonable results. \n",
        "\n",
        "*   Use high-res image. The model is trained with 1024x1024 images. Use at least 512x512 with fine-details. Low-res images and JPEG artifacts may result in unsatisfactory results. \n",
        "*   Use an image with a single person. If the image contain multiple people, reconstruction quality is likely degraded.\n",
        "*   Front facing with standing works best (or with fashion pose)\n",
        "*   The entire body is covered within the image. (Note: now missing legs is partially supported)\n",
        "*   Make sure the input image is well lit. Exteremy dark or bright image and strong shadow often create artifacts.\n",
        "*   I recommend nearly parallel camera angle to the ground. High camera height may result in distorted legs or high heels. \n",
        "*   If the background is cluttered, use less complex background or try removing it using https://www.remove.bg/ before processing.\n",
        "*   It's trained with human only. Any other characters may not work well.\n",
        "*   Search on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag to get a better sense of what succeeds and what fails. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bl6J9QGxaMq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl6zw63iGQaI",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "#@markdown Mount Google Drive to install PIFuHD, load images and to save the results.\n",
        "\n",
        "#@markdown After running this cell you will get the link. Follow the link, grant access to your Drive and copy auth code.\n",
        "\n",
        "#@markdown Paste the code to the input below and press Enter\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmpEwdOd1G1z",
        "cellView": "form"
      },
      "source": [
        "#@title Installation\n",
        "#@markdown This cell will download PIFuHD and Human Pose Estimator which are needed to reconstruct 3D model from single photo of a human\n",
        "!mkdir /content/images\n",
        "# download pifuhd\n",
        "!git clone https://github.com/facebookresearch/pifuhd\n",
        "%cd /content/pifuhd\n",
        "!sh ./scripts/download_trained_model.sh\n",
        "%cd ..\n",
        "# download human pose estimator\n",
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_RKMlk-xmOS",
        "cellView": "form"
      },
      "source": [
        "#@title Configure\n",
        "#@markdown Here wee configure the pathes to input and output data.\n",
        "\n",
        "#@markdown Upload images either to your Google Drive or to Colab directly and paste the path here\n",
        "image_path = \"/content/images\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown Provide the path to the directory where you want to store your results. It's better to save results on Google Drive because you can access those files later.\n",
        "result_path = \"/content/drive/MyDrive/current-workshop/results/pifuhd\" #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbVmda9J5TDL"
      },
      "source": [
        "## Preprocess images\n",
        "\n",
        "Before we start reconstruction we need to preprocess our images. We will use the Lightweight Human Pose Estimation neural network for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRcDXe38lHB",
        "cellView": "form"
      },
      "source": [
        "#@title Initialize preprocessing code\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "import demo\n",
        "import glob\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "            \n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "              pmin = valid_keypoints.min(0)\n",
        "              pmax = valid_keypoints.max(0)\n",
        "\n",
        "              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n",
        "              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "              # if leg is missing, use pelvis to get cropping\n",
        "              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n",
        "              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n",
        "              center[1] += int(0.05*radius)\n",
        "            else:\n",
        "              center = np.array([img.shape[1]//2,img.shape[0]//2])\n",
        "              radius = max(img.shape[1]//2,img.shape[0]//2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2*radius, 2*radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cGZD6f6IaY",
        "cellView": "form"
      },
      "source": [
        "#@title Run preprocessing\n",
        "images = []\n",
        "for ext in ('png', 'PNG', 'jpg', 'jpeg', 'JPG', 'JPEG'):\n",
        "  images.extend(glob.glob(f\"{image_path}/*.{ext}\"))\n",
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "\n",
        "get_rect(net.cuda(), images, 512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8vUzT3M00_H"
      },
      "source": [
        "## Reconstruction\n",
        "\n",
        "Now we are ready to run PIFuHD to reconstruct the 3d model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5995t2PnQTmG",
        "cellView": "form"
      },
      "source": [
        "#@title Run PIFuHD\n",
        "%cd /content/pifuhd\n",
        "# Warning: all images with the corresponding rectangle files under -i will be processed. \n",
        "!python -m apps.simple_test -r 256 --use_rect -i $image_path -o $result_path\n",
        "\n",
        "# seems that 256 is the maximum resolution that can fit into Google Colab. \n",
        "# If you want to reconstruct a higher-resolution mesh, please try with your own machine. "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}